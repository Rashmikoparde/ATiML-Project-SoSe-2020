{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block is for all the libraries that the program require.\n",
    "import math\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import pandas as pa\n",
    "import os \n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "import textstat\n",
    "#nltk.download('wordnet')\n",
    "from gensim.test.utils import common_texts,get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tag import RegexpTagger\n",
    "from collections import OrderedDict \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os \n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "#Preprocessing packages\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from gensim.test.utils import common_texts,get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "#Import necessary Pakages\n",
    "from os import listdir\n",
    "import os\n",
    "import itertools\n",
    "import sklearn\n",
    "import inflect\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block contains from reading teh master file to loading content from teh directory into teh dataframe.\n",
    "\n",
    "def load_nd_save(master_file_path, books_file_path):\n",
    "    meta_data = pa.read_csv(master_file_path,sep = \";\", engine = 'python', \n",
    "                            header = 0, encoding = 'ISO-8859-1')\n",
    "\n",
    "    #meta_data['book_id'].isna().sum() # to check if there are any  na values in book_id column\n",
    "\n",
    "    # removing the epub extension\n",
    "    meta_data['book_id'] = meta_data['book_id'].replace(regex=['.epub'],value= '-content.html')\n",
    "\n",
    "    # Loading the content into data frames\n",
    "    files_dir = books_file_path\n",
    "    for i in meta_data.index:\n",
    "        try:\n",
    "            f = str(meta_data.loc[i,'book_id'])\n",
    "            soup = BeautifulSoup(open(files_dir + f,encoding=\"ISO-8859-1\"),'html.parser')\n",
    "            meta_data.loc[i,'content'] = soup.text\n",
    "        except:\n",
    "            continue\n",
    "    meta_data = meta_data.dropna(axis = 0)\n",
    "    meta_data = meta_data.replace(np.nan, 0)\n",
    "    print(meta_data)\n",
    "    return (meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub('\\[[^]]*\\]', '', text) #remove_between_square_brackets(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    new_words = [] \n",
    "    #new_words = list([new_words.append(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')) for word in words])\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    words = [c.lower() for c in new_words]\n",
    "    getVals = list([val for val in words if val.isalpha()])\n",
    "    words = getVals\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    #new_words = list([new_words.append(word) for word in words if word not in stopwords.words('english')])\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemmas = []\n",
    "    #for word in new_words:\n",
    "        #lemma = lemmatizer.lemmatize(word)\n",
    "        #lemmas.append(lemma)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# This block will pre-process for each book and save it into a dictionary with \n",
    "#all the books and it's tokens and in a dataframe as a one string as well\n",
    "def call_preprocess(no_of_books, meta_data):\n",
    "    # pre-processing first 50 books\n",
    "    a = meta_data.loc[0:no_of_books,]\n",
    "    dit = {}\n",
    "    for i in a.index:\n",
    "        words = preprocess(a.loc[i,'content'])\n",
    "        dit[a.loc[i,'book_id']] = list(words)\n",
    "        a.loc[i,'preprocessed_content'] = ' '.join(map(str,words))\n",
    "    return(a,dit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(a, no_of_books):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    # Extracting few features\n",
    "    # calculating total no of words in each book\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    a['content_word_count'] = a['content'].apply(lambda x: len(x.strip().split()))\n",
    "\n",
    "    # calculating unique no of words in each book\n",
    "    a['content_unique_words']= a['content'].apply(lambda x:len(set(str(x).split())))\n",
    "\n",
    "    # calculating the readbility  score here\n",
    "    a['flesch_score'] = a['content'].apply(lambda x : textstat.flesch_reading_ease(x))\n",
    "    \n",
    "    a['pre-processed_unique_words']= a['preprocessed_content'].apply(lambda x:len(set(str(x).split(\" \"))))\n",
    "    #writing_style2 = pa.DataFrame(columns = ['noun','comma','Cardinal_nos','hyphen','Plural_nouns','Past_Tense_Verbs','Adverbs','Exclamation','Articles','Male_Pronoun','Personal_Pronoun','Gerund','Semicolon','Colon','Adjectives','Interjections','Possessive_Pronoun','Female_Pronoun'])\n",
    "    \n",
    "    # calculating teh no of sentences in each book\n",
    "    a['no_sentences'] = a['content'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "    \n",
    "    li = []\n",
    "    cordConj_l = []\n",
    "    subConj_l = []\n",
    "    senti_l = pa.DataFrame()\n",
    "    period_l = []\n",
    "    char_l = []\n",
    "    for i in a.index:\n",
    "        regexp_tagger = RegexpTagger([ (r'(aha|ahem|ahh|ahoy|alas|arg|aw|bam|bingo|blah|boo|bravo|brrr|cheers|congratulations|dang|drat|darn|duh|eek|eh|encore|eureka|fiddlesticks|gadzooks|gee|gee|whiz|golly|goodbye|good|grief|gosh|Shh|Please|please|Psst|Shoo|Hey|Oh|Yo|Here|Ahem|Encore|Hush|Scat|No|Silence|Enough|Yuck|Eww|Aww|Ouch|Oh|Ah|Ugh|Phew|Phooey|Rats|Yippee|Aw|Blah|Brr|Eek|Good|Grief|Alas|Bingo|Bravo|Eureka|Crikey|Gee|Golly|Gosh|Hmm|Holy|cow|Aha|Oh|Huh|Duh|Ahh|Well)$','a') , # Interjections\n",
    "            (r'^-?[0-9]+(.[0-9]+)?$', 'b'), #  for cardinal numbers\n",
    "            (r'(The|the|A|a|An|an)$', 'c'),  # for articles\n",
    "             (r'.*able$', 'd'),                # adjectives\n",
    "             (r'.*ly$', 'e'),                  # adverbs\n",
    "             (r'.*s$', 'f'),                   # plural nouns\n",
    "              (r'.*ing$', 'g'),                # gerunds\n",
    "              (r'.*ed$', 'h'),                 # past tense verbs\n",
    "              (r'(She|she|her|Her)$', 'i') ,  # female pronouns\n",
    "             (r'(he|HIM|him|his|HIS|Him|His|He)$', 'j'), # Male pronouns\n",
    "             (r'(I|i|you|You|It|it|We|we|They|they)$', 'k'), # Personal Pronoun\n",
    "             (r'(Mine|Mine|yours|Yours|Hers|hers|Theirs|theirs|their|Their|Its|its|Ours|ours|Ones)$', 'l'), # Possessive Pronoun   \n",
    "                (r'-', 'm'),\n",
    "                (r';','n'),\n",
    "                (r':','o'),\n",
    "                (r'!', 'p'),\n",
    "                (r',','q'),\n",
    "                (r'(.*)','r')  # nouns, basically whatever is left \n",
    "            ])\n",
    "        x = nltk.word_tokenize(a.loc[i,'content'])\n",
    "        ######### Mr. chandan's part\n",
    "        #words_Sent = nltk.word_tokenize(merged_df['content'].values[i])\n",
    "        words_Sent = np.unique(x)\n",
    "        num = 0.5*len(np.unique(words_Sent))\n",
    "        words_Sent = random.sample(list(words_Sent), int(num))\n",
    "        temp = ' '.join(map(str,words_Sent))\n",
    "        #for j in words_Sent: \n",
    "         #   temp = temp+(j)+\" \"\n",
    "        sent = analyser.polarity_scores(temp)\n",
    "        senti_l = senti_l.append(sent, ignore_index= True) \n",
    "        #merged_df['sent_scores'].values[i] = sent\n",
    "        ########### now Priyanka's part\n",
    "        y = nltk.pos_tag(x)\n",
    "        CCcnt=0\n",
    "        subCnt=0\n",
    "        cordConj = [item for item in y if 'CC' in item]\n",
    "        subConj = [item for item in y if 'IN' in item]\n",
    "        cordConj_l.append(len(cordConj))\n",
    "        subConj_l.append(len(subConj))\n",
    "        tk = regexp_tagger.tag(x)\n",
    "        tag_fd = nltk.FreqDist(tag for (word, tag) in tk)\n",
    "        sorted_items = OrderedDict(sorted(tag_fd.items())) \n",
    "        li.append(sorted_items.values())\n",
    "        period_l.append(a.loc[i,'content'].count('.'))\n",
    "        ############# Mr Chandan part 2\n",
    "        def process_words(words):\n",
    "            ps = PorterStemmer()\n",
    "            # convert to lower case\n",
    "            words = [w.lower() for w in words]\n",
    "            words = [word for word in words if word.isalpha()]\n",
    "            stems = [ps.stem(w) for w in words]\n",
    "            return np.unique(stems)\n",
    "\n",
    "        #for i in range(len(merged_df)):\n",
    "        doc = nlp(a.loc[i,'content'])\n",
    "        # Text and label of named entity span\n",
    "        taggings = pa.DataFrame([(ent.text, ent.label_) for ent in doc.ents])\n",
    "        taggings.columns = ['name','tag']\n",
    "        tags = taggings[taggings.tag==\"PERSON\"]\n",
    "        counts = dict(Counter(tags.name))\n",
    "        chars = []\n",
    "        for val in counts:\n",
    "            if(counts[val]>1):\n",
    "                chars.append(val)\n",
    "        char_l.append(len(process_words(np.unique(chars))))\n",
    "        \n",
    "        \n",
    "        ########################\n",
    "        \n",
    "    writing_style = pa.DataFrame(li)\n",
    "    #writing_style.columns = sorted_items.keys()\n",
    "    #changing the column names of writing style dataframe\n",
    "    writing_style.columns = ['Interjections','Cardinal_Nums','Articles','Adjectives','Adverbs','Plural_Nouns','Gerunds','Past_tense_verbs','Female_Pronoun','Male_Pronoun','Personal_Pronoun','Possessive_Pronoun','Hyphen','semi-colon','colon','Exclamation','Comma','Noun']\n",
    "    writing_style['coordinating_conjunctions'] = cordConj_l\n",
    "    writing_style['subcord_conjunctions'] = subConj_l\n",
    "    writing_style['char_count'] = char_l\n",
    "    #writing_style['sent_scores'] = senti_l\n",
    "    writing_style['period'] = period_l\n",
    "    final_df = pa.concat([writing_style,senti_l], axis = 1)\n",
    "    return a,final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates word vectors\n",
    "def wordvectors(dit):    \n",
    "    # calculating word vectors for all the books\n",
    "    my_df = pa.DataFrame()\n",
    "    for key in dit:\n",
    "        d = [dit[key]]\n",
    "        word2veca = gensim.models.Word2Vec([dit[key]] ,min_count = 1, size = 15, window = 8, sg = 1 , seed = 123, negative = 0, iter = 100, workers = 10)\n",
    "        words = pa.DataFrame(word2veca.wv.vectors, index=word2veca.wv.index2word)\n",
    "        words = words.values.sum(axis = 0) / words.shape[0]\n",
    "        my_df.loc[:,str(key)] = words\n",
    "\n",
    "    f = my_df.T\n",
    "    f.reset_index(level=0, inplace=True)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_vectors = pa.DataFrame(word2veca.wv.vectors, index=word2veca.wv.index2word)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_append(writing_style,f, a, no_of_books):\n",
    "    # appending all the other fetaures to teh word2vec\n",
    "    f['label'] =  a.loc[0:no_of_books,'guten_genre']\n",
    "    f['flesch_score'] = a.loc[0:no_of_books,'flesch_score']\n",
    "    f['token_ratio'] = a.loc[0:no_of_books,'content_unique_words']/a.loc[0:no_of_books,'content_word_count']\n",
    "    f['no_of_sentences'] = a.loc[0:no_of_books,'no_sentences']\n",
    "    gum = pa.concat([f,writing_style],axis = 1)\n",
    "    gum = gum.replace(np.nan, 0)\n",
    "    print(gum)\n",
    "    return gum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_predict(gum):\n",
    "    cols2exclude = [0,16,21,39] # These are teh features that we have excluded and not decided to train and base our model on.\n",
    "    y = gum.loc[:,'label']\n",
    "    X = preprocessing.scale(gum.iloc[:, ~gum.columns.isin(gum.columns[cols2exclude])])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3, random_state = 0)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    train_pred = gnb.fit(X_train,y_train).predict(X_train)\n",
    "    print(\"********* Following results on test data ***********\")\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "    bn = y_test.tolist()\n",
    "    for i in range(len(y_pred)):\n",
    "          print(\"actual genre is \",bn[i],\"but predicted is \",y_pred[i])\n",
    "\n",
    "\n",
    "    print(\"f1 score is \", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    print( \"********** Following data on Training data *********\")\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (X_train.shape[0], (y_train != train_pred).sum()))\n",
    "    for i in range(len(train_pred)):\n",
    "          print(\"actual genre is \",y_train.tolist()[i],\"but predicted is \",train_pred[i])\n",
    "    print(\"f1 score is \", f1_score(y_train, train_pred, average='weighted'))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Book_Name  \\\n",
      "0    The Mystery of the Boule Cabinet: A Detective ...   \n",
      "1                                            The Pupil   \n",
      "2                                       At Love's Cost   \n",
      "3                               The Heart of the Range   \n",
      "4                          The Worshipper of the Image   \n",
      "..                                                 ...   \n",
      "991                                  David Copperfield   \n",
      "992                                         Hard Times   \n",
      "993                          Memoirs of Shelock Holmes   \n",
      "994                    The Mysterious Affair at Styles   \n",
      "995                               A Tale of Two Cities   \n",
      "\n",
      "                                  book_id            guten_genre  \\\n",
      "0                    pg10067-content.html  Detective and Mystery   \n",
      "1                     pg1032-content.html               Literary   \n",
      "2                    pg10379-content.html               Literary   \n",
      "3                    pg10473-content.html        Western Stories   \n",
      "4                    pg10812-content.html               Literary   \n",
      "..                                    ...                    ...   \n",
      "991  pg766DickensDavidCopfld-content.html               Literary   \n",
      "992    pg786DickensHardTimes-content.html               Literary   \n",
      "993  pg834DoyleMemoirsSherlk-content.html  Detective and Mystery   \n",
      "994             pg863Agatha1-content.html  Detective and Mystery   \n",
      "995    pg98DickensTaleCities-content.html               Literary   \n",
      "\n",
      "                  Author_Name  \\\n",
      "0    Stevenson| Burton Egbert   \n",
      "1                James| Henry   \n",
      "2            Garvice| Charles   \n",
      "3    White| William Patterson   \n",
      "4       Gallienne| Richard Le   \n",
      "..                        ...   \n",
      "991          Dickens| Charles   \n",
      "992          Dickens| Charles   \n",
      "993             Connan| Doyle   \n",
      "994          Christie| Agatha   \n",
      "995          Dickens| Charles   \n",
      "\n",
      "                                               content  \n",
      "0    A Detective Story\\nA.B.M. Fellow-Sherlockian\\n...  \n",
      "1    This edition first published 1916\\nThe text fo...  \n",
      "2    \"Until this moment I have never fully realised...  \n",
      "3    \"The Rider of Golden Bar,\" \"Hidden Trails,\" \"L...  \n",
      "4    The Worshipper of the Image\\nEvening was in th...  \n",
      "..                                                 ...  \n",
      "991  I do not find it easy to get sufficiently far ...  \n",
      "992  The One Thing Needful\\nMurdering the Innocents...  \n",
      "993  \"I am afraid, Watson, that I shall have to go,...  \n",
      "994  The intense interest aroused in the public by ...  \n",
      "995  !!!!Â Â Book the First - Recalled to Life I. Â...  \n",
      "\n",
      "[996 rows x 5 columns]\n",
      "successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:844: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully pre-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully extracted features\n",
      "                  index         0         1         2         3         4  \\\n",
      "0  pg10067-content.html -0.000131 -0.000459  0.000205  0.000471 -0.000475   \n",
      "1   pg1032-content.html -0.000518 -0.000597  0.000658  0.000299  0.000658   \n",
      "2  pg10379-content.html -0.000173  0.000168  0.000191  0.000336 -0.000009   \n",
      "\n",
      "          5         6         7         8  ...   Comma     Noun  \\\n",
      "0 -0.000361 -0.000123 -0.000053  0.000400  ...   53067      0.0   \n",
      "1  0.000119 -0.000070 -0.000061  0.000394  ...     962  12579.0   \n",
      "2 -0.000210 -0.000187 -0.000090  0.000099  ...  110445      0.0   \n",
      "\n",
      "   coordinating_conjunctions  subcord_conjunctions  char_count  period  \\\n",
      "0                       2668                  7753          29    4367   \n",
      "1                        569                  2266           8     823   \n",
      "2                       7157                 17707          57    7137   \n",
      "\n",
      "  compound    neg    neu    pos  \n",
      "0  -0.9819  0.216  0.577  0.207  \n",
      "1   0.9994  0.137  0.651  0.212  \n",
      "2  -0.9991  0.224  0.575  0.201  \n",
      "\n",
      "[3 rows x 46 columns]\n",
      "********* Following results on test data ***********\n",
      "Number of mislabeled points out of a total 1 points : 1\n",
      "actual genre is  Literary but predicted is  Detective and Mystery\n",
      "f1 score is  0.0\n",
      "********** Following data on Training data *********\n",
      "Number of mislabeled points out of a total 2 points : 0\n",
      "actual genre is  Literary but predicted is  Literary\n",
      "actual genre is  Detective and Mystery but predicted is  Detective and Mystery\n",
      "f1 score is  1.0\n",
      "None\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "master_file_path =  \"D:/DKE-sem/sem2/AML/project/master996n.csv\"\n",
    "\n",
    "books_file_path = \"D:/DKE-sem/sem2/AML/project/Gutenberg_English_Fiction_1k/Gutenberg_19th_century_English_Fiction_full/\"\n",
    "no_of_books = 2\n",
    "meta_data = load_nd_save(master_file_path, books_file_path)\n",
    "print(\"successfully loaded\")\n",
    "a, dit = call_preprocess(no_of_books, meta_data)\n",
    "print(\"successfully pre-processed\")\n",
    "a, writing_style = features(a, no_of_books)\n",
    "print(\"successfully extracted features\")\n",
    "f = wordvectors(dit)\n",
    "\n",
    "gum = features_append(writing_style,f, a, no_of_books)\n",
    "\n",
    "\n",
    "X = train_n_predict(gum)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Noun</th>\n",
       "      <th>coordinating_conjunctions</th>\n",
       "      <th>subcord_conjunctions</th>\n",
       "      <th>char_count</th>\n",
       "      <th>period</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pg10067-content.html</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.000361</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>...</td>\n",
       "      <td>53067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2668</td>\n",
       "      <td>7753</td>\n",
       "      <td>29</td>\n",
       "      <td>4367</td>\n",
       "      <td>-0.9819</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pg1032-content.html</td>\n",
       "      <td>-0.000518</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>...</td>\n",
       "      <td>962</td>\n",
       "      <td>12579.0</td>\n",
       "      <td>569</td>\n",
       "      <td>2266</td>\n",
       "      <td>8</td>\n",
       "      <td>823</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pg10379-content.html</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>...</td>\n",
       "      <td>110445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7157</td>\n",
       "      <td>17707</td>\n",
       "      <td>57</td>\n",
       "      <td>7137</td>\n",
       "      <td>-0.9991</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index         0         1         2         3         4  \\\n",
       "0  pg10067-content.html -0.000131 -0.000459  0.000205  0.000471 -0.000475   \n",
       "1   pg1032-content.html -0.000518 -0.000597  0.000658  0.000299  0.000658   \n",
       "2  pg10379-content.html -0.000173  0.000168  0.000191  0.000336 -0.000009   \n",
       "\n",
       "          5         6         7         8  ...   Comma     Noun  \\\n",
       "0 -0.000361 -0.000123 -0.000053  0.000400  ...   53067      0.0   \n",
       "1  0.000119 -0.000070 -0.000061  0.000394  ...     962  12579.0   \n",
       "2 -0.000210 -0.000187 -0.000090  0.000099  ...  110445      0.0   \n",
       "\n",
       "   coordinating_conjunctions  subcord_conjunctions  char_count  period  \\\n",
       "0                       2668                  7753          29    4367   \n",
       "1                        569                  2266           8     823   \n",
       "2                       7157                 17707          57    7137   \n",
       "\n",
       "  compound    neg    neu    pos  \n",
       "0  -0.9819  0.216  0.577  0.207  \n",
       "1   0.9994  0.137  0.651  0.212  \n",
       "2  -0.9991  0.224  0.575  0.201  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                    'index',                           0,\n",
       "                                 1,                           2,\n",
       "                                 3,                           4,\n",
       "                                 5,                           6,\n",
       "                                 7,                           8,\n",
       "                                 9,                          10,\n",
       "                                11,                          12,\n",
       "                                13,                          14,\n",
       "                           'label',              'flesch_score',\n",
       "                     'token_ratio',           'no_of_sentences',\n",
       "                   'Interjections',             'Cardinal_Nums',\n",
       "                        'Articles',                'Adjectives',\n",
       "                         'Adverbs',              'Plural_Nouns',\n",
       "                         'Gerunds',          'Past_tense_verbs',\n",
       "                  'Female_Pronoun',              'Male_Pronoun',\n",
       "                'Personal_Pronoun',        'Possessive_Pronoun',\n",
       "                          'Hyphen',                'semi-colon',\n",
       "                           'colon',               'Exclamation',\n",
       "                           'Comma',                      'Noun',\n",
       "       'coordinating_conjunctions',      'subcord_conjunctions',\n",
       "                      'char_count',                    'period',\n",
       "                        'compound',                       'neg',\n",
       "                             'neu',                       'pos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gum.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
