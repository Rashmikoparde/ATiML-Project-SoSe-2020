{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block is for all the libraries that the program require.\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pa\n",
    "import os \n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import inflect\n",
    "\n",
    "import textstat\n",
    "#nltk.download('wordnet')\n",
    "from gensim.test.utils import common_texts,get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tag import RegexpTagger\n",
    "from collections import OrderedDict \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block contains from reading teh master file to loading content from teh directory into teh dataframe.\n",
    "\n",
    "def load_nd_save(master_file_path, books_file_path):\n",
    "    meta_data = pa.read_csv(master_file_path,sep = \";\", engine = 'python', \n",
    "                            header = 0, encoding = 'ISO-8859-1')\n",
    "\n",
    "    #meta_data['book_id'].isna().sum() # to check if there are any  na values in book_id column\n",
    "\n",
    "    # removing the epub extension\n",
    "    meta_data['book_id'] = meta_data['book_id'].replace(regex=['.epub'],value= '-content.html')\n",
    "\n",
    "    # Loading the content into data frames\n",
    "    files_dir = books_file_path\n",
    "    for i in meta_data.index:\n",
    "        try:\n",
    "            f = str(meta_data.loc[i,'book_id'])\n",
    "            soup = BeautifulSoup(open(files_dir + f,encoding=\"ISO-8859-1\"),'html.parser')\n",
    "            meta_data.loc[i,'content'] = soup.text\n",
    "        except:\n",
    "            continue\n",
    "    meta_data = meta_data.dropna(axis = 0)\n",
    "    meta_data = meta_data.replace(np.nan, 0)\n",
    "    return (meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub('\\[[^]]*\\]', '', text) #remove_between_square_brackets(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    new_words = [] \n",
    "    #new_words = list([new_words.append(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')) for word in words])\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    words = [c.lower() for c in new_words]\n",
    "    getVals = list([val for val in words if val.isalpha()])\n",
    "    words = getVals\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    #new_words = list([new_words.append(word) for word in words if word not in stopwords.words('english')])\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemmas = []\n",
    "    #for word in new_words:\n",
    "        #lemma = lemmatizer.lemmatize(word)\n",
    "        #lemmas.append(lemma)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# This block will pre-process for each book and save it into a dictionary with \n",
    "#all the books and it's tokens and in a dataframe as a one string as well\n",
    "def call_preprocess(no_of_books, meta_data):\n",
    "    # pre-processing first 50 books\n",
    "    a = meta_data.loc[0:no_of_books,]\n",
    "    dit = {}\n",
    "    for i in a.index:\n",
    "        words = preprocess(a.loc[i,'content'])\n",
    "        dit[a.loc[i,'book_id']] = list(words)\n",
    "        a.loc[i,'preprocessed_content'] = ' '.join(map(str,words))\n",
    "    return(a,dit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(a, no_of_books):\n",
    "    # Extracting few features\n",
    "    # calculating total no of words in each book\n",
    "    a['content_word_count'] = a['content'].apply(lambda x: len(x.strip().split()))\n",
    "\n",
    "    # calculating unique no of words in each book\n",
    "    a['content_unique_words']= a['content'].apply(lambda x:len(set(str(x).split())))\n",
    "\n",
    "    # calculating the readbility  score here\n",
    "    a['flesch_score'] = a['content'].apply(lambda x : textstat.flesch_reading_ease(x))\n",
    "    \n",
    "    a['pre-processed_unique_words']= a['preprocessed_content'].apply(lambda x:len(set(str(x).split(\" \"))))\n",
    "    #writing_style2 = pa.DataFrame(columns = ['noun','comma','Cardinal_nos','hyphen','Plural_nouns','Past_Tense_Verbs','Adverbs','Exclamation','Articles','Male_Pronoun','Personal_Pronoun','Gerund','Semicolon','Colon','Adjectives','Interjections','Possessive_Pronoun','Female_Pronoun'])\n",
    "\n",
    "    li = []\n",
    "    for i in a.index:\n",
    "        regexp_tagger = RegexpTagger([ (r'(aha|ahem|ahh|ahoy|alas|arg|aw|bam|bingo|blah|boo|bravo|brrr|cheers|congratulations|dang|drat|darn|duh|eek|eh|encore|eureka|fiddlesticks|gadzooks|gee|gee|whiz|golly|goodbye|good|grief|gosh|Shh|Please|please|Psst|Shoo|Hey|Oh|Yo|Here|Ahem|Encore|Hush|Scat|No|Silence|Enough|Yuck|Eww|Aww|Ouch|Oh|Ah|Ugh|Phew|Phooey|Rats|Yippee|Aw|Blah|Brr|Eek|Good|Grief|Alas|Bingo|Bravo|Eureka|Crikey|Gee|Golly|Gosh|Hmm|Holy|cow|Aha|Oh|Huh|Duh|Ahh|Well)$','a') , # Interjections\n",
    "            (r'^-?[0-9]+(.[0-9]+)?$', 'b'), #  for cardinal numbers\n",
    "            (r'(The|the|A|a|An|an)$', 'c'),  # for articles\n",
    "             (r'.*able$', 'd'),                # adjectives\n",
    "             (r'.*ly$', 'e'),                  # adverbs\n",
    "             (r'.*s$', 'f'),                   # plural nouns\n",
    "              (r'.*ing$', 'g'),                # gerunds\n",
    "              (r'.*ed$', 'h'),                 # past tense verbs\n",
    "              (r'(She|she|her|Her)$', 'i') ,  # female pronouns\n",
    "             (r'(he|HIM|him|his|HIS|Him|His|He)$', 'j'), # Male pronouns\n",
    "             (r'(I|i|you|You|It|it|We|we|They|they)$', 'k'), # Personal Pronoun\n",
    "             (r'(Mine|Mine|yours|Yours|Hers|hers|Theirs|theirs|their|Their|Its|its|Ours|ours|Ones)$', 'l'), # Possessive Pronoun   \n",
    "                (r'-', 'm'),\n",
    "                (r';','n'),\n",
    "                (r':','o'),\n",
    "                (r'!', 'p'),\n",
    "                (r',','q'),\n",
    "                (r'(.*)','r')  # nouns, basically whatever is left \n",
    "            ])\n",
    "        tk = regexp_tagger.tag(nltk.word_tokenize(a.loc[i,'content']))\n",
    "        tag_fd = nltk.FreqDist(tag for (word, tag) in tk)\n",
    "        sorted_items = OrderedDict(sorted(tag_fd.items())) \n",
    "        li.append(sorted_items.values())\n",
    "    writing_style = pa.DataFrame(li)\n",
    "    #writing_style.columns = sorted_items.keys()\n",
    "    #changing the column names of writing style dataframe\n",
    "    writing_style.columns = ['Interjections','Cardinal_Nums','Articles','Adjectives','Adverbs','Plural_Nouns','Gerunds','Past_tense_verbs','Female_Pronoun','Male_Pronoun','Personal_Pronoun','Possessive_Pronoun','Hyphen','semi-colon','colon','Exclamation','Comma','Noun']\n",
    "    \n",
    "    return a,writing_style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates word vectors\n",
    "def wordvectors(dit):    \n",
    "    # calculating word vectors for all the books\n",
    "    my_df = pa.DataFrame()\n",
    "    for key in dit:\n",
    "        d = [dit[key]]\n",
    "        word2veca = gensim.models.Word2Vec([dit[key]] ,min_count = 1, size = 15, window = 8, sg = 1 , seed = 123, negative = 0, iter = 100, workers = 10)\n",
    "        words = pa.DataFrame(word2veca.wv.vectors, index=word2veca.wv.index2word)\n",
    "        words = words.values.sum(axis = 0) / words.shape[0]\n",
    "        my_df.loc[:,str(key)] = words\n",
    "\n",
    "    f = my_df.T\n",
    "    f.reset_index(level=0, inplace=True)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_vectors = pa.DataFrame(word2veca.wv.vectors, index=word2veca.wv.index2word)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_append(writing_style,f, a, no_of_books):\n",
    "    # appending all the other fetaures to teh word2vec\n",
    "    f['label'] =  a.loc[0:no_of_books,'guten_genre']\n",
    "    f['flesch_score'] = a.loc[0:no_of_books,'flesch_score']\n",
    "    f['token_ratio'] = a.loc[0:no_of_books,'content_unique_words']/a.loc[0:no_of_books,'content_word_count']\n",
    "    gum = pa.concat([f,writing_style],axis = 1)\n",
    "    gum = gum.replace(np.nan, 0)\n",
    "    return gum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_predict(gum):\n",
    "    cols2exclude = [0,16,20,36] # These are teh features that we have excluded and not decided to train and base our model on.\n",
    "    y = gum.loc[:,'label']\n",
    "    X = preprocessing.scale(gum.iloc[:, ~gum.columns.isin(gum.columns[cols2exclude])])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3, random_state = 0)\n",
    "\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    train_pred = gnb.fit(X_train,y_train).predict(X_train)\n",
    "    print(\"********* Following results on test data ***********\")\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "    bn = y_test.tolist()\n",
    "    for i in range(len(y_pred)):\n",
    "          print(\"actual genre is \",bn[i],\"but predicted is \",y_pred[i])\n",
    "\n",
    "\n",
    "    print(\"f1 score is \", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    print( \"********** Following data on Training data *********\")\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (X_train.shape[0], (y_train != train_pred).sum()))\n",
    "    for i in range(len(train_pred)):\n",
    "          print(\"actual genre is \",y_train.tolist()[i],\"but predicted is \",train_pred[i])\n",
    "    print(\"f1 score is \", f1_score(y_train, train_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:844: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "master_file_path =  \"D:/DKE-sem/sem2/AML/project/master996n.csv\"\n",
    "\n",
    "books_file_path = \"D:/DKE-sem/sem2/AML/project/Gutenberg_English_Fiction_1k/Gutenberg_19th_century_English_Fiction_full/\"\n",
    "no_of_books = 50\n",
    "meta_data = load_nd_save(master_file_path, books_file_path)\n",
    "print(\"successfully loaded\")\n",
    "a, dit = call_preprocess(no_of_books, meta_data)\n",
    "print(\"successfully pre-processed\")\n",
    "a, writing_style = features(a, no_of_books)\n",
    "print(\"successfully extracted features\")\n",
    "f = wordvectors(dit)\n",
    "\n",
    "gum = features_append(writing_style,f, a, no_of_books)\n",
    "\n",
    "train_n_predict(gum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
